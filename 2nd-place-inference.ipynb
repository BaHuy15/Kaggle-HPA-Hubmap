{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03a13578",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-22T01:21:24.609150Z",
     "iopub.status.busy": "2022-09-22T01:21:24.608598Z",
     "iopub.status.idle": "2022-09-22T01:21:59.810215Z",
     "shell.execute_reply": "2022-09-22T01:21:59.809218Z"
    },
    "papermill": {
     "duration": 35.209158,
     "end_time": "2022-09-22T01:21:59.812893",
     "exception": false,
     "start_time": "2022-09-22T01:21:24.603735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/einops-041-wheel/einops-0.4.1-py3-none-any.whl\r\n",
      "Installing collected packages: einops\r\n",
      "Successfully installed einops-0.4.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install /kaggle/input/einops-041-wheel/einops-0.4.1-py3-none-any.whl\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\n",
    "sys.path.append('../input/subweights2')\n",
    "import timm\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:64\"\n",
    "\n",
    "from os import path, makedirs, listdir\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.backends import cudnn\n",
    "cudnn.benchmark = True\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import timeit\n",
    "import cv2\n",
    "t0 = timeit.default_timer()\n",
    "\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8df197c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-22T01:21:59.819121Z",
     "iopub.status.busy": "2022-09-22T01:21:59.818409Z",
     "iopub.status.idle": "2022-09-22T01:21:59.836985Z",
     "shell.execute_reply": "2022-09-22T01:21:59.836044Z"
    },
    "papermill": {
     "duration": 0.023726,
     "end_time": "2022-09-22T01:21:59.839066",
     "exception": false,
     "start_time": "2022-09-22T01:21:59.815340",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = '.'\n",
    "data_dir = '../input/hubmap-organ-segmentation/'\n",
    "# models_folder = 'weights'\n",
    "models_folder = '../input/subweights3/'\n",
    "models_folder1 = '../input/subweights4/'\n",
    "models_folder2 = '../input/subweights2/'\n",
    "\n",
    "df = pd.read_csv(path.join(data_dir, 'test.csv'))\n",
    "\n",
    "organs = ['prostate', 'spleen', 'lung', 'kidney', 'largeintestine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4679626b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-22T01:21:59.844638Z",
     "iopub.status.busy": "2022-09-22T01:21:59.844203Z",
     "iopub.status.idle": "2022-09-22T01:21:59.883748Z",
     "shell.execute_reply": "2022-09-22T01:21:59.882863Z"
    },
    "papermill": {
     "duration": 0.044719,
     "end_time": "2022-09-22T01:21:59.885816",
     "exception": false,
     "start_time": "2022-09-22T01:21:59.841097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rle_encode_less_memory(img):\n",
    "    pixels = img.T.flatten()\n",
    "    pixels[0] = 0\n",
    "    pixels[-1] = 0\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "\n",
    "def preprocess_inputs(x):\n",
    "    x = np.asarray(x, dtype='float32')\n",
    "    x /= 127\n",
    "    x -= 1\n",
    "    return x\n",
    "\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, data_dir='test_images', new_size=None):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.data_dir = data_dir\n",
    "        self.new_size = new_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        r = self.df.iloc[idx]\n",
    "\n",
    "        img0 = cv2.imread(path.join(self.data_dir, '{}.tiff'.format(r['id'])), cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "        orig_shape = img0.shape\n",
    "\n",
    "        sample = {'id': r['id'], 'organ': r['organ'], 'data_source': r['data_source'], 'orig_h': orig_shape[0], 'orig_w': orig_shape[1]}\n",
    "\n",
    "        for i in range(len(self.new_size)):\n",
    "\n",
    "            img = cv2.resize(img0, self.new_size[i])\n",
    "\n",
    "            img = preprocess_inputs(img)\n",
    "            img = torch.from_numpy(img.transpose((2, 0, 1)).copy()).float()\n",
    "\n",
    "            sample['img{}'.format(i)] = img\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "class ConvSilu(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3):\n",
    "        super(ConvSilu, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, padding=1),\n",
    "            nn.SiLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "\n",
    "\n",
    "from coat import *\n",
    "\n",
    "\n",
    "class Timm_Unet(nn.Module):\n",
    "    def __init__(self, name='resnet34', pretrained=True, inp_size=3, otp_size=1, decoder_filters=[32, 48, 64, 96, 128], **kwargs):\n",
    "        super(Timm_Unet, self).__init__()\n",
    "\n",
    "        if name.startswith('coat'):\n",
    "            encoder = coat_lite_medium()\n",
    "\n",
    "            if pretrained:\n",
    "                checkpoint = './weights/coat_lite_medium_384x384_f9129688.pth'\n",
    "                checkpoint = torch.load(checkpoint, map_location=lambda storage, loc: storage)\n",
    "                state_dict = checkpoint['model']\n",
    "                encoder.load_state_dict(state_dict,strict=False)\n",
    "        \n",
    "            encoder_filters = encoder.embed_dims\n",
    "        else:\n",
    "            encoder = timm.create_model(name, features_only=True, pretrained=pretrained, in_chans=inp_size)\n",
    "\n",
    "            encoder_filters = [f['num_chs'] for f in encoder.feature_info]\n",
    "\n",
    "        decoder_filters = decoder_filters\n",
    "\n",
    "        self.conv6 = ConvSilu(encoder_filters[-1], decoder_filters[-1])\n",
    "        self.conv6_2 = ConvSilu(decoder_filters[-1] + encoder_filters[-2], decoder_filters[-1])\n",
    "        self.conv7 = ConvSilu(decoder_filters[-1], decoder_filters[-2])\n",
    "        self.conv7_2 = ConvSilu(decoder_filters[-2] + encoder_filters[-3], decoder_filters[-2])\n",
    "        self.conv8 = ConvSilu(decoder_filters[-2], decoder_filters[-3])\n",
    "        self.conv8_2 = ConvSilu(decoder_filters[-3] + encoder_filters[-4], decoder_filters[-3])\n",
    "        self.conv9 = ConvSilu(decoder_filters[-3], decoder_filters[-4])\n",
    "\n",
    "        if len(encoder_filters) == 4:\n",
    "            self.conv9_2 = None\n",
    "        else:\n",
    "            self.conv9_2 = ConvSilu(decoder_filters[-4] + encoder_filters[-5], decoder_filters[-4])\n",
    "        \n",
    "        self.conv10 = ConvSilu(decoder_filters[-4], decoder_filters[-5])\n",
    "        \n",
    "        self.res = nn.Conv2d(decoder_filters[-5], otp_size, 1, stride=1, padding=0)\n",
    "\n",
    "        self.cls =  nn.Linear(encoder_filters[-1] * 2, 5)\n",
    "        self.pix_sz =  nn.Linear(encoder_filters[-1] * 2, 1)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "        self.encoder = encoder\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, C, H, W = x.shape\n",
    "\n",
    "        if self.conv9_2 is None:\n",
    "            enc2, enc3, enc4, enc5 = self.encoder(x)\n",
    "        else:\n",
    "            enc1, enc2, enc3, enc4, enc5 = self.encoder(x)\n",
    "\n",
    "        dec6 = self.conv6(F.interpolate(enc5, scale_factor=2))\n",
    "        dec6 = self.conv6_2(torch.cat([dec6, enc4\n",
    "                ], 1))\n",
    "\n",
    "        dec7 = self.conv7(F.interpolate(dec6, scale_factor=2))\n",
    "        dec7 = self.conv7_2(torch.cat([dec7, enc3\n",
    "                ], 1))\n",
    "        \n",
    "        dec8 = self.conv8(F.interpolate(dec7, scale_factor=2))\n",
    "        dec8 = self.conv8_2(torch.cat([dec8, enc2\n",
    "                ], 1))\n",
    "\n",
    "        dec9 = self.conv9(F.interpolate(dec8, scale_factor=2))\n",
    "\n",
    "        if self.conv9_2 is not None:\n",
    "            dec9 = self.conv9_2(torch.cat([dec9, \n",
    "                    enc1\n",
    "                    ], 1))\n",
    "        \n",
    "        dec10 = self.conv10(dec9) # F.interpolate(dec9, scale_factor=2))\n",
    "\n",
    "        x1 = torch.cat([F.adaptive_avg_pool2d(enc5, output_size=1).view(batch_size, -1), \n",
    "                        F.adaptive_max_pool2d(enc5, output_size=1).view(batch_size, -1)], 1)\n",
    "\n",
    "        # x1 = F.dropout(x1, p=0.3, training=self.training)\n",
    "        organ_cls = self.cls(x1)\n",
    "        pixel_size = self.pix_sz(x1)\n",
    "\n",
    "        return self.res(dec10), organ_cls, pixel_size\n",
    "\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Linear):\n",
    "                m.weight.data = nn.init.kaiming_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d4dcf3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-22T01:21:59.892242Z",
     "iopub.status.busy": "2022-09-22T01:21:59.891950Z",
     "iopub.status.idle": "2022-09-22T01:28:52.462014Z",
     "shell.execute_reply": "2022-09-22T01:28:52.460629Z"
    },
    "papermill": {
     "duration": 412.576079,
     "end_time": "2022-09-22T01:28:52.464538",
     "exception": false,
     "start_time": "2022-09-22T01:21:59.888459",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'size': (768, 768), 'models': [('tf_efficientnet_b7_ns', 'tf_efficientnet_b7_ns_768_e34_{}_best', '../input/subweights3/', 1), ('convnext_large_384_in22ft1k', 'convnext_large_384_in22ft1k_768_e37_{}_best', '../input/subweights3/', 1), ('tf_efficientnetv2_l_in21ft1k', 'tf_efficientnetv2_l_in21ft1k_768_e36_{}_best', '../input/subweights3/', 1), ('coat_lite_medium', 'coat_lite_medium_768_e40_{}_best', '../input/subweights2/', 3)], 'pred_dir': 'test_pred_768', 'weight': 0.2}\n",
      "=> loading checkpoint 'tf_efficientnet_b7_ns_768_e34_0_best'\n",
      "loaded checkpoint 'tf_efficientnet_b7_ns_768_e34_0_best' (epoch 131, best_score 0.8078845670425936)\n",
      "=> loading checkpoint 'tf_efficientnet_b7_ns_768_e34_1_best'\n",
      "loaded checkpoint 'tf_efficientnet_b7_ns_768_e34_1_best' (epoch 107, best_score 0.8443877185206096)\n",
      "=> loading checkpoint 'tf_efficientnet_b7_ns_768_e34_2_best'\n",
      "loaded checkpoint 'tf_efficientnet_b7_ns_768_e34_2_best' (epoch 46, best_score 0.8203977918772558)\n",
      "=> loading checkpoint 'tf_efficientnet_b7_ns_768_e34_3_best'\n",
      "loaded checkpoint 'tf_efficientnet_b7_ns_768_e34_3_best' (epoch 156, best_score 0.8124184530735699)\n",
      "=> loading checkpoint 'tf_efficientnet_b7_ns_768_e34_4_best'\n",
      "loaded checkpoint 'tf_efficientnet_b7_ns_768_e34_4_best' (epoch 161, best_score 0.8727498012180679)\n",
      "=> loading checkpoint 'convnext_large_384_in22ft1k_768_e37_0_best'\n",
      "loaded checkpoint 'convnext_large_384_in22ft1k_768_e37_0_best' (epoch 77, best_score 0.8017890882483585)\n",
      "=> loading checkpoint 'convnext_large_384_in22ft1k_768_e37_1_best'\n",
      "loaded checkpoint 'convnext_large_384_in22ft1k_768_e37_1_best' (epoch 70, best_score 0.8419051080913922)\n",
      "=> loading checkpoint 'convnext_large_384_in22ft1k_768_e37_2_best'\n",
      "loaded checkpoint 'convnext_large_384_in22ft1k_768_e37_2_best' (epoch 88, best_score 0.8434267701766742)\n",
      "=> loading checkpoint 'convnext_large_384_in22ft1k_768_e37_3_best'\n",
      "loaded checkpoint 'convnext_large_384_in22ft1k_768_e37_3_best' (epoch 71, best_score 0.8434140536973092)\n",
      "=> loading checkpoint 'convnext_large_384_in22ft1k_768_e37_4_best'\n",
      "loaded checkpoint 'convnext_large_384_in22ft1k_768_e37_4_best' (epoch 92, best_score 0.8756650096690796)\n",
      "=> loading checkpoint 'tf_efficientnetv2_l_in21ft1k_768_e36_0_best'\n",
      "loaded checkpoint 'tf_efficientnetv2_l_in21ft1k_768_e36_0_best' (epoch 110, best_score 0.8297024069334828)\n",
      "=> loading checkpoint 'tf_efficientnetv2_l_in21ft1k_768_e36_1_best'\n",
      "loaded checkpoint 'tf_efficientnetv2_l_in21ft1k_768_e36_1_best' (epoch 136, best_score 0.8540758077542696)\n",
      "=> loading checkpoint 'tf_efficientnetv2_l_in21ft1k_768_e36_2_best'\n",
      "loaded checkpoint 'tf_efficientnetv2_l_in21ft1k_768_e36_2_best' (epoch 87, best_score 0.8465945342530856)\n",
      "=> loading checkpoint 'tf_efficientnetv2_l_in21ft1k_768_e36_3_best'\n",
      "loaded checkpoint 'tf_efficientnetv2_l_in21ft1k_768_e36_3_best' (epoch 143, best_score 0.8169841588616655)\n",
      "=> loading checkpoint 'tf_efficientnetv2_l_in21ft1k_768_e36_4_best'\n",
      "loaded checkpoint 'tf_efficientnetv2_l_in21ft1k_768_e36_4_best' (epoch 122, best_score 0.8687130183113474)\n",
      "=> loading checkpoint 'coat_lite_medium_768_e40_0_best'\n",
      "loaded checkpoint 'coat_lite_medium_768_e40_0_best' (epoch 74, best_score 0.8524178527067857)\n",
      "=> loading checkpoint 'coat_lite_medium_768_e40_1_best'\n",
      "loaded checkpoint 'coat_lite_medium_768_e40_1_best' (epoch 94, best_score 0.8658902376085924)\n",
      "=> loading checkpoint 'coat_lite_medium_768_e40_2_best'\n",
      "loaded checkpoint 'coat_lite_medium_768_e40_2_best' (epoch 46, best_score 0.8524739666312589)\n",
      "=> loading checkpoint 'coat_lite_medium_768_e40_3_best'\n",
      "loaded checkpoint 'coat_lite_medium_768_e40_3_best' (epoch 63, best_score 0.8237604846721212)\n",
      "=> loading checkpoint 'coat_lite_medium_768_e40_4_best'\n",
      "loaded checkpoint 'coat_lite_medium_768_e40_4_best' (epoch 131, best_score 0.8890967255856232)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:21<00:00, 21.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10078 spleen [4.1636445e-09 1.0000000e+00 1.3623394e-09 1.0364132e-09 2.1940867e-09] [0.1115]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'size': (1024, 1024), 'models': [('convnext_large_384_in22ft1k', 'convnext_large_384_in22ft1k_1024_e32_{}_best', '../input/subweights2/', 1), ('tf_efficientnet_b7_ns', 'tf_efficientnet_b7_ns_1024_e33_{}_best', '../input/subweights3/', 1), ('tf_efficientnetv2_l_in21ft1k', 'tf_efficientnetv2_l_in21ft1k_1024_e38_{}_best', '../input/subweights3/', 1), ('coat_lite_medium', 'coat_lite_medium_1024_e41_{}_best', '../input/subweights3/', 3)], 'pred_dir': 'test_pred_1024', 'weight': 0.3}\n",
      "=> loading checkpoint 'convnext_large_384_in22ft1k_1024_e32_0_best'\n",
      "loaded checkpoint 'convnext_large_384_in22ft1k_1024_e32_0_best' (epoch 47, best_score 0.7805124423859635)\n",
      "=> loading checkpoint 'convnext_large_384_in22ft1k_1024_e32_1_best'\n",
      "loaded checkpoint 'convnext_large_384_in22ft1k_1024_e32_1_best' (epoch 77, best_score 0.8244463664991708)\n",
      "=> loading checkpoint 'convnext_large_384_in22ft1k_1024_e32_2_best'\n",
      "loaded checkpoint 'convnext_large_384_in22ft1k_1024_e32_2_best' (epoch 57, best_score 0.8459396654649436)\n",
      "=> loading checkpoint 'convnext_large_384_in22ft1k_1024_e32_3_best'\n",
      "loaded checkpoint 'convnext_large_384_in22ft1k_1024_e32_3_best' (epoch 42, best_score 0.7960767223764675)\n",
      "=> loading checkpoint 'convnext_large_384_in22ft1k_1024_e32_4_best'\n",
      "loaded checkpoint 'convnext_large_384_in22ft1k_1024_e32_4_best' (epoch 62, best_score 0.867448888352498)\n",
      "=> loading checkpoint 'tf_efficientnet_b7_ns_1024_e33_0_best'\n",
      "loaded checkpoint 'tf_efficientnet_b7_ns_1024_e33_0_best' (epoch 114, best_score 0.8256274680634929)\n",
      "=> loading checkpoint 'tf_efficientnet_b7_ns_1024_e33_1_best'\n",
      "loaded checkpoint 'tf_efficientnet_b7_ns_1024_e33_1_best' (epoch 100, best_score 0.8470782011695519)\n",
      "=> loading checkpoint 'tf_efficientnet_b7_ns_1024_e33_2_best'\n",
      "loaded checkpoint 'tf_efficientnet_b7_ns_1024_e33_2_best' (epoch 130, best_score 0.849776941696387)\n",
      "=> loading checkpoint 'tf_efficientnet_b7_ns_1024_e33_3_best'\n",
      "loaded checkpoint 'tf_efficientnet_b7_ns_1024_e33_3_best' (epoch 107, best_score 0.819187284943423)\n",
      "=> loading checkpoint 'tf_efficientnet_b7_ns_1024_e33_4_best'\n",
      "loaded checkpoint 'tf_efficientnet_b7_ns_1024_e33_4_best' (epoch 127, best_score 0.8758689886444294)\n",
      "=> loading checkpoint 'tf_efficientnetv2_l_in21ft1k_1024_e38_0_best'\n",
      "loaded checkpoint 'tf_efficientnetv2_l_in21ft1k_1024_e38_0_best' (epoch 88, best_score 0.8191903698173792)\n",
      "=> loading checkpoint 'tf_efficientnetv2_l_in21ft1k_1024_e38_1_best'\n",
      "loaded checkpoint 'tf_efficientnetv2_l_in21ft1k_1024_e38_1_best' (epoch 129, best_score 0.8568609582309769)\n",
      "=> loading checkpoint 'tf_efficientnetv2_l_in21ft1k_1024_e38_2_best'\n",
      "loaded checkpoint 'tf_efficientnetv2_l_in21ft1k_1024_e38_2_best' (epoch 82, best_score 0.8503092778544056)\n",
      "=> loading checkpoint 'tf_efficientnetv2_l_in21ft1k_1024_e38_3_best'\n",
      "loaded checkpoint 'tf_efficientnetv2_l_in21ft1k_1024_e38_3_best' (epoch 58, best_score 0.8354736732023262)\n",
      "=> loading checkpoint 'tf_efficientnetv2_l_in21ft1k_1024_e38_4_best'\n",
      "loaded checkpoint 'tf_efficientnetv2_l_in21ft1k_1024_e38_4_best' (epoch 98, best_score 0.8618372949620796)\n",
      "=> loading checkpoint 'coat_lite_medium_1024_e41_0_best'\n",
      "loaded checkpoint 'coat_lite_medium_1024_e41_0_best' (epoch 37, best_score 0.842305555167788)\n",
      "=> loading checkpoint 'coat_lite_medium_1024_e41_1_best'\n",
      "loaded checkpoint 'coat_lite_medium_1024_e41_1_best' (epoch 31, best_score 0.8646169319282935)\n",
      "=> loading checkpoint 'coat_lite_medium_1024_e41_2_best'\n",
      "loaded checkpoint 'coat_lite_medium_1024_e41_2_best' (epoch 61, best_score 0.8480485419239796)\n",
      "=> loading checkpoint 'coat_lite_medium_1024_e41_3_best'\n",
      "loaded checkpoint 'coat_lite_medium_1024_e41_3_best' (epoch 67, best_score 0.8373222599900656)\n",
      "=> loading checkpoint 'coat_lite_medium_1024_e41_4_best'\n",
      "loaded checkpoint 'coat_lite_medium_1024_e41_4_best' (epoch 93, best_score 0.8883505520448337)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:20<00:00, 20.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10078 spleen [1.37591405e-08 1.00000000e+00 1.17440990e-09 1.50797219e-09\n",
      " 3.61743724e-09] [0.05313]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:20<00:00, 20.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'size': (1472, 1472), 'models': [('tf_efficientnet_b7_ns', 'tf_efficientnet_b7_ns_1472_e35_{}_best', '../input/subweights3/', 1), ('tf_efficientnetv2_l_in21ft1k', 'tf_efficientnetv2_l_in21ft1k_1472_e39_{}_best', '../input/subweights3/', 1), ('coat_lite_medium', 'coat_lite_medium_1472_e42_{}_best', '../input/subweights2/', 3)], 'pred_dir': 'test_pred_1472', 'weight': 0.5}\n",
      "=> loading checkpoint 'tf_efficientnet_b7_ns_1472_e35_0_best'\n",
      "loaded checkpoint 'tf_efficientnet_b7_ns_1472_e35_0_best' (epoch 84, best_score 0.8071029944957873)\n",
      "=> loading checkpoint 'tf_efficientnet_b7_ns_1472_e35_1_best'\n",
      "loaded checkpoint 'tf_efficientnet_b7_ns_1472_e35_1_best' (epoch 130, best_score 0.853828850025396)\n",
      "=> loading checkpoint 'tf_efficientnet_b7_ns_1472_e35_2_best'\n",
      "loaded checkpoint 'tf_efficientnet_b7_ns_1472_e35_2_best' (epoch 102, best_score 0.8518945842944202)\n",
      "=> loading checkpoint 'tf_efficientnet_b7_ns_1472_e35_3_best'\n",
      "loaded checkpoint 'tf_efficientnet_b7_ns_1472_e35_3_best' (epoch 98, best_score 0.816828890292755)\n",
      "=> loading checkpoint 'tf_efficientnet_b7_ns_1472_e35_4_best'\n",
      "loaded checkpoint 'tf_efficientnet_b7_ns_1472_e35_4_best' (epoch 108, best_score 0.8770144127606716)\n",
      "=> loading checkpoint 'tf_efficientnetv2_l_in21ft1k_1472_e39_0_best'\n",
      "loaded checkpoint 'tf_efficientnetv2_l_in21ft1k_1472_e39_0_best' (epoch 115, best_score 0.8500323483212189)\n",
      "=> loading checkpoint 'tf_efficientnetv2_l_in21ft1k_1472_e39_1_best'\n",
      "loaded checkpoint 'tf_efficientnetv2_l_in21ft1k_1472_e39_1_best' (epoch 107, best_score 0.8615813369305082)\n",
      "=> loading checkpoint 'tf_efficientnetv2_l_in21ft1k_1472_e39_2_best'\n",
      "loaded checkpoint 'tf_efficientnetv2_l_in21ft1k_1472_e39_2_best' (epoch 109, best_score 0.8572309934943441)\n",
      "=> loading checkpoint 'tf_efficientnetv2_l_in21ft1k_1472_e39_3_best'\n",
      "loaded checkpoint 'tf_efficientnetv2_l_in21ft1k_1472_e39_3_best' (epoch 85, best_score 0.8238956941076988)\n",
      "=> loading checkpoint 'tf_efficientnetv2_l_in21ft1k_1472_e39_4_best'\n",
      "loaded checkpoint 'tf_efficientnetv2_l_in21ft1k_1472_e39_4_best' (epoch 98, best_score 0.8745398517903806)\n",
      "=> loading checkpoint 'coat_lite_medium_1472_e42_0_best'\n",
      "loaded checkpoint 'coat_lite_medium_1472_e42_0_best' (epoch 72, best_score 0.8582187075786941)\n",
      "=> loading checkpoint 'coat_lite_medium_1472_e42_1_best'\n",
      "loaded checkpoint 'coat_lite_medium_1472_e42_1_best' (epoch 54, best_score 0.8791907922644296)\n",
      "=> loading checkpoint 'coat_lite_medium_1472_e42_2_best'\n",
      "loaded checkpoint 'coat_lite_medium_1472_e42_2_best' (epoch 71, best_score 0.8501591390426155)\n",
      "=> loading checkpoint 'coat_lite_medium_1472_e42_3_best'\n",
      "loaded checkpoint 'coat_lite_medium_1472_e42_3_best' (epoch 96, best_score 0.8370773398576667)\n",
      "=> loading checkpoint 'coat_lite_medium_1472_e42_4_best'\n",
      "loaded checkpoint 'coat_lite_medium_1472_e42_4_best' (epoch 82, best_score 0.8840726780939459)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:26<00:00, 26.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10078 spleen [3.5346909e-07 9.9999893e-01 5.3994910e-08 3.8820983e-07 3.4125836e-07] [0.056]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 6.878 min\n"
     ]
    }
   ],
   "source": [
    "test_batch_size = 1\n",
    "\n",
    "# amp_autocast = suppress\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "\n",
    "half_size = True\n",
    "\n",
    "hubmap_only = False #True #False\n",
    "\n",
    "\n",
    "organ_threshold = {\n",
    "    'Hubmap': {\n",
    "        'kidney'        : 90,\n",
    "        'prostate'      : 100,\n",
    "        'largeintestine': 80,\n",
    "        'spleen'        : 100,\n",
    "        'lung'          : 15,\n",
    "    },\n",
    "    'HPA': {\n",
    "        'kidney'        : 127,\n",
    "        'prostate'      : 127,\n",
    "        'largeintestine': 127,\n",
    "        'spleen'        : 127,\n",
    "        'lung'          : 25,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "params = [\n",
    "    {'size': (768, 768), 'models': [\n",
    "                                    ('tf_efficientnet_b7_ns', 'tf_efficientnet_b7_ns_768_e34_{}_best', models_folder, 1), \n",
    "                                    ('convnext_large_384_in22ft1k', 'convnext_large_384_in22ft1k_768_e37_{}_best', models_folder, 1),\n",
    "                                    ('tf_efficientnetv2_l_in21ft1k', 'tf_efficientnetv2_l_in21ft1k_768_e36_{}_best', models_folder, 1), \n",
    "                                    ('coat_lite_medium', 'coat_lite_medium_768_e40_{}_best', models_folder2, 3),\n",
    "                                   ],\n",
    "                         'pred_dir': 'test_pred_768', 'weight': 0.2},\n",
    "    {'size': (1024, 1024), 'models': [\n",
    "                                      ('convnext_large_384_in22ft1k', 'convnext_large_384_in22ft1k_1024_e32_{}_best', models_folder2, 1), \n",
    "                                      ('tf_efficientnet_b7_ns', 'tf_efficientnet_b7_ns_1024_e33_{}_best', models_folder, 1),\n",
    "                                      ('tf_efficientnetv2_l_in21ft1k', 'tf_efficientnetv2_l_in21ft1k_1024_e38_{}_best', models_folder, 1),\n",
    "                                    ('coat_lite_medium', 'coat_lite_medium_1024_e41_{}_best', models_folder, 3),\n",
    "                                   ],\n",
    "                         'pred_dir': 'test_pred_1024', 'weight': 0.3},\n",
    "    {'size': (1472, 1472), 'models': [\n",
    "                                    ('tf_efficientnet_b7_ns', 'tf_efficientnet_b7_ns_1472_e35_{}_best', models_folder, 1),\n",
    "                                    ('tf_efficientnetv2_l_in21ft1k', 'tf_efficientnetv2_l_in21ft1k_1472_e39_{}_best', models_folder, 1),\n",
    "                                    ('coat_lite_medium', 'coat_lite_medium_1472_e42_{}_best', models_folder2, 3),\n",
    "                                   ],\n",
    "                         'pred_dir': 'test_pred_1472', 'weight': 0.5},\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "def predict_models(param):\n",
    "    print(param)\n",
    "\n",
    "    makedirs(param['pred_dir'], exist_ok=True)\n",
    "\n",
    "    models = []\n",
    "\n",
    "    test_data = TestDataset(df, path.join(data_dir, 'test_images'), new_size=[param['size']])\n",
    "\n",
    "    test_data_loader = DataLoader(test_data, batch_size=test_batch_size, num_workers=1, shuffle=False)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    for model_name, checkpoint_name, checkpoint_dir, model_weight in param['models']:\n",
    "        for fold in range(5):\n",
    "            model = Timm_Unet(name=model_name, pretrained=None)\n",
    "            snap_to_load = checkpoint_name.format(fold)\n",
    "            print(\"=> loading checkpoint '{}'\".format(snap_to_load))\n",
    "            checkpoint = torch.load(path.join(checkpoint_dir, snap_to_load), map_location='cpu')\n",
    "            loaded_dict = checkpoint['state_dict']\n",
    "            sd = model.state_dict()\n",
    "            for k in model.state_dict():\n",
    "                if k in loaded_dict:\n",
    "                    sd[k] = loaded_dict[k]\n",
    "            loaded_dict = sd\n",
    "            model.load_state_dict(loaded_dict)\n",
    "            print(\"loaded checkpoint '{}' (epoch {}, best_score {})\".format(snap_to_load, \n",
    "                checkpoint['epoch'], checkpoint['best_score']))\n",
    "            model = model.eval().cuda()\n",
    "\n",
    "            models.append((model, model_weight))\n",
    "\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        for sample in tqdm(test_data_loader):\n",
    "            \n",
    "            ids = sample[\"id\"].cpu().numpy()\n",
    "            orig_w = sample[\"orig_w\"].cpu().numpy()\n",
    "            orig_h = sample[\"orig_h\"].cpu().numpy()\n",
    "            # pixel_size = sample[\"pixel_size\"].cpu().numpy()\n",
    "            organ = sample[\"organ\"]\n",
    "            data_source = sample[\"data_source\"]\n",
    "\n",
    "            \n",
    "            if hubmap_only and (data_source[0] != 'Hubmap'):\n",
    "                continue\n",
    "\n",
    "\n",
    "            msk_preds = []\n",
    "            for i in range(0, len(ids), 1):\n",
    "                msk_preds.append(np.zeros((orig_h[i], orig_w[i]), dtype='float32'))\n",
    "\n",
    "            cnt = 0\n",
    "\n",
    "            imgs = sample[\"img0\"].cpu().numpy()\n",
    "\n",
    "            with amp_autocast():\n",
    "                for _tta in range(4): #8\n",
    "                    _i = _tta // 2\n",
    "                    _flip = False\n",
    "                    if _tta % 2 == 1:\n",
    "                        _flip = True\n",
    "\n",
    "                    if _i == 0:\n",
    "                        inp = imgs.copy()\n",
    "                    elif _i == 1:\n",
    "                        inp = np.rot90(imgs, k=1, axes=(2,3)).copy()\n",
    "                    elif _i == 2:\n",
    "                        inp = np.rot90(imgs, k=2, axes=(2,3)).copy()\n",
    "                    elif _i == 3:\n",
    "                        inp = np.rot90(imgs, k=3, axes=(2,3)).copy()\n",
    "\n",
    "                    if _flip:\n",
    "                        inp = inp[:, :, :, ::-1].copy()\n",
    "\n",
    "                    inp = torch.from_numpy(inp).float().cuda()                   \n",
    "                    \n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "                    for model, model_weight in models:\n",
    "                        out, res_cls, res_pix = model(inp)\n",
    "                        msk_pred = torch.sigmoid(out).cpu().numpy()\n",
    "                        \n",
    "                        res_cls = torch.softmax(res_cls, dim=1).cpu().numpy()\n",
    "                        res_pix = res_pix.cpu().numpy()\n",
    "                        \n",
    "                        if _flip:\n",
    "                            msk_pred = msk_pred[:, :, :, ::-1].copy()\n",
    "\n",
    "                        if _i == 1:\n",
    "                            msk_pred = np.rot90(msk_pred, k=4-1, axes=(2,3)).copy()\n",
    "                        elif _i == 2:\n",
    "                            msk_pred = np.rot90(msk_pred, k=4-2, axes=(2,3)).copy()\n",
    "                        elif _i == 3:\n",
    "                            msk_pred = np.rot90(msk_pred, k=4-3, axes=(2,3)).copy()\n",
    "\n",
    "                        cnt += model_weight\n",
    "\n",
    "                        for i in range(len(ids)):\n",
    "                            msk_preds[i] += model_weight * cv2.resize(msk_pred[i, 0].astype('float32'), (orig_w[i], orig_h[i]))\n",
    "\n",
    "                    del inp\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "            for i in range(len(ids)):\n",
    "                msk_pred = msk_preds[i] / cnt\n",
    "                msk_pred = (msk_pred * 255).astype('uint8')\n",
    "\n",
    "                print(ids[i], organ[i], res_cls[i], res_pix[i]) #pixel_size[i]\n",
    "\n",
    "                cv2.imwrite(path.join(param['pred_dir'] , '{}.png'.format(ids[i])), msk_pred, [cv2.IMWRITE_PNG_COMPRESSION, 4])\n",
    "\n",
    "    del models\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "for param in params:\n",
    "    predict_models(param)\n",
    "\n",
    "\n",
    "\n",
    "res_df = []\n",
    "\n",
    "for _, r in df.iterrows():\n",
    "    preds = []\n",
    "\n",
    "    if hubmap_only and (r['data_source'] != 'Hubmap'):\n",
    "        res_df.append({'id': r['id'], 'rle': ''})\n",
    "        continue\n",
    "\n",
    "    for param in params:\n",
    "        pred = cv2.imread(path.join(param['pred_dir'], '{}.png'.format(r['id'])), cv2.IMREAD_GRAYSCALE)\n",
    "        preds.append(pred * param['weight'])\n",
    "\n",
    "    _thr = organ_threshold[r['data_source']][r['organ']]\n",
    "\n",
    "    pred = np.asarray(preds).sum(axis=0)\n",
    "\n",
    "    res_df.append({'id': r['id'], 'rle': rle_encode_less_memory(pred > _thr)})\n",
    "\n",
    "    # cv2.imwrite(path.join('.', '{}.png'.format(r['id'])), pred.astype('uint8'), [cv2.IMWRITE_PNG_COMPRESSION, 4])\n",
    "\n",
    "res_df = pd.DataFrame(res_df)\n",
    "res_df.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "elapsed = timeit.default_timer() - t0\n",
    "print('Time: {:.3f} min'.format(elapsed / 60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 458.631871,
   "end_time": "2022-09-22T01:28:54.911546",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-09-22T01:21:16.279675",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
